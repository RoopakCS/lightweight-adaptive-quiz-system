{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9495492",
   "metadata": {},
   "source": [
    "# Lightweight Adaptive Quiz System (LAQS) - Interactive Exploration\n",
    "\n",
    "This notebook demonstrates how to use the LAQS system programmatically and explore the results interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452e595",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043234ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import LAQS components\n",
    "from src import (\n",
    "    QuestionBank,\n",
    "    AdaptiveEngine,\n",
    "    NonAdaptiveEngine,\n",
    "    SimulatedLearner,\n",
    "    LearnerPopulation,\n",
    "    QuizSimulator,\n",
    "    ExperimentRunner,\n",
    "    PerformanceTracker,\n",
    "    VisualizationEngine,\n",
    "    StatisticalAnalyzer\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd8ca2",
   "metadata": {},
   "source": [
    "## 2. Explore Question Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create question bank\n",
    "qb = QuestionBank(num_questions=100)\n",
    "\n",
    "# View statistics\n",
    "stats = qb.get_statistics()\n",
    "print(\"Question Bank Statistics:\")\n",
    "print(f\"Total Questions: {stats['total_questions']}\")\n",
    "print(f\"\\nBy Difficulty: {stats['by_difficulty']}\")\n",
    "print(f\"\\nBy Topic: {stats['by_topic']}\")\n",
    "print(f\"\\nAverage Expected Time: {stats['avg_expected_time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions\n",
    "print(\"Sample Easy Question:\")\n",
    "print(qb.get_question(difficulty='easy'))\n",
    "print(\"\\nSample Hard Question:\")\n",
    "print(qb.get_question(difficulty='hard'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da669f9",
   "metadata": {},
   "source": [
    "## 3. Create and Test Learner Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different learner types\n",
    "struggling_learner = SimulatedLearner(\"struggling\", base_ability=0.4, learning_rate=0.05)\n",
    "average_learner = SimulatedLearner(\"average\", base_ability=0.6, learning_rate=0.05)\n",
    "advanced_learner = SimulatedLearner(\"advanced\", base_ability=0.85, learning_rate=0.03)\n",
    "\n",
    "print(\"Learner Profiles:\")\n",
    "for learner in [struggling_learner, average_learner, advanced_learner]:\n",
    "    profile = learner.get_profile()\n",
    "    print(f\"\\n{profile['learner_id'].upper()}:\")\n",
    "    print(f\"  Base Ability: {profile['base_ability']:.2f}\")\n",
    "    print(f\"  Learning Rate: {profile['learning_rate']:.3f}\")\n",
    "    print(f\"  Speed Factor: {profile['speed_factor']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a614a2",
   "metadata": {},
   "source": [
    "## 4. Test Adaptive Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a998c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adaptive engine\n",
    "engine = AdaptiveEngine(\n",
    "    accuracy_threshold_high=0.80,\n",
    "    accuracy_threshold_low=0.50,\n",
    "    time_threshold_factor=1.0,\n",
    "    window_size=5\n",
    ")\n",
    "\n",
    "# Simulate high performance\n",
    "high_performance = [\n",
    "    {'correct': True, 'time': 15, 'expected_time': 20, 'difficulty': 'easy'},\n",
    "    {'correct': True, 'time': 18, 'expected_time': 20, 'difficulty': 'easy'},\n",
    "    {'correct': True, 'time': 16, 'expected_time': 20, 'difficulty': 'easy'},\n",
    "    {'correct': True, 'time': 17, 'expected_time': 20, 'difficulty': 'easy'},\n",
    "    {'correct': True, 'time': 19, 'expected_time': 20, 'difficulty': 'easy'},\n",
    "]\n",
    "\n",
    "next_diff = engine.get_next_difficulty(high_performance, 'easy')\n",
    "print(f\"After high performance on easy questions: {next_diff}\")\n",
    "\n",
    "# Simulate low performance\n",
    "low_performance = [\n",
    "    {'correct': False, 'time': 50, 'expected_time': 40, 'difficulty': 'hard'},\n",
    "    {'correct': False, 'time': 55, 'expected_time': 40, 'difficulty': 'hard'},\n",
    "    {'correct': True, 'time': 48, 'expected_time': 40, 'difficulty': 'hard'},\n",
    "]\n",
    "\n",
    "next_diff = engine.get_next_difficulty(low_performance, 'hard')\n",
    "print(f\"After low performance on hard questions: {next_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743f7b7",
   "metadata": {},
   "source": [
    "## 5. Run Mini Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create components\n",
    "qb = QuestionBank(num_questions=50)\n",
    "population = LearnerPopulation(num_learners=3)\n",
    "runner = ExperimentRunner(qb, num_questions_per_session=10)\n",
    "\n",
    "# Run experiment\n",
    "print(\"Running mini experiment...\")\n",
    "results = runner.run_population_experiment(population, num_sessions=2)\n",
    "print(\"✓ Experiment complete!\")\n",
    "\n",
    "# Get summary\n",
    "summary = runner.get_summary_statistics()\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a61a7b",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tracker\n",
    "tracker = runner.get_tracker()\n",
    "\n",
    "# Load session data\n",
    "sessions_df = tracker.get_all_sessions_df()\n",
    "\n",
    "print(\"Session Data Sample:\")\n",
    "display(sessions_df.head())\n",
    "\n",
    "print(f\"\\nTotal Sessions: {len(sessions_df)}\")\n",
    "print(f\"Adaptive Sessions: {len(sessions_df[sessions_df['quiz_type'] == 'adaptive'])}\")\n",
    "print(f\"Non-Adaptive Sessions: {len(sessions_df[sessions_df['quiz_type'] == 'non-adaptive'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis\n",
    "analyzer = StatisticalAnalyzer(tracker)\n",
    "\n",
    "# Descriptive stats\n",
    "desc_stats = analyzer.compute_descriptive_stats()\n",
    "print(\"Descriptive Statistics:\")\n",
    "for quiz_type, stats in desc_stats.items():\n",
    "    print(f\"\\n{quiz_type.upper()}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['accuracy']['mean']:.4f}\")\n",
    "    print(f\"  Mean Mastery: {stats['mastery_index']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1674640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-tests\n",
    "t_tests = analyzer.perform_t_tests()\n",
    "print(\"T-test Results:\")\n",
    "for metric, result in t_tests.items():\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  t-statistic: {result['t_statistic']:.4f}\")\n",
    "    print(f\"  p-value: {result['p_value']:.6f}\")\n",
    "    print(f\"  Significant: {result['significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e8407",
   "metadata": {},
   "source": [
    "## 7. Custom Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "sessions_df.boxplot(column='accuracy', by='quiz_type', ax=ax[0])\n",
    "ax[0].set_title('Accuracy Distribution')\n",
    "ax[0].set_xlabel('Quiz Type')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "plt.sca(ax[0])\n",
    "plt.xticks([1, 2], ['Adaptive', 'Non-Adaptive'])\n",
    "\n",
    "# Bar chart\n",
    "mean_accuracy = sessions_df.groupby('quiz_type')['accuracy'].mean()\n",
    "mean_accuracy.plot(kind='bar', ax=ax[1], color=['#2ecc71', '#e74c3c'])\n",
    "ax[1].set_title('Mean Accuracy Comparison')\n",
    "ax[1].set_xlabel('Quiz Type')\n",
    "ax[1].set_ylabel('Mean Accuracy')\n",
    "ax[1].set_xticklabels(['Adaptive', 'Non-Adaptive'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cfe8ad",
   "metadata": {},
   "source": [
    "## 8. Load and Analyze Existing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29adf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files (if they exist)\n",
    "try:\n",
    "    sessions = pd.read_csv('data/session_results.csv')\n",
    "    questions = pd.read_csv('data/question_responses.csv')\n",
    "    \n",
    "    print(\"Loaded existing data:\")\n",
    "    print(f\"Sessions: {len(sessions)}\")\n",
    "    print(f\"Questions: {len(questions)}\")\n",
    "    \n",
    "    # Quick analysis\n",
    "    print(\"\\nQuick Analysis:\")\n",
    "    print(sessions.groupby('quiz_type')[['accuracy', 'mastery_index']].mean())\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing data files found. Run main.py first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7f9e7",
   "metadata": {},
   "source": [
    "## 9. Custom Learner Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f895db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom learner and run through both quiz types\n",
    "custom_learner = SimulatedLearner(\n",
    "    learner_id=\"custom_001\",\n",
    "    base_ability=0.65,\n",
    "    learning_rate=0.06,\n",
    "    speed_factor=1.1,\n",
    "    consistency=0.85\n",
    ")\n",
    "\n",
    "qb = QuestionBank(num_questions=50)\n",
    "simulator = QuizSimulator(qb)\n",
    "tracker = PerformanceTracker()\n",
    "\n",
    "# Run adaptive session\n",
    "print(\"Running adaptive quiz...\")\n",
    "adaptive_result = simulator.run_quiz_session(\n",
    "    custom_learner, \n",
    "    num_questions=15, \n",
    "    quiz_type='adaptive',\n",
    "    tracker=tracker,\n",
    "    session_id='custom_adaptive'\n",
    ")\n",
    "\n",
    "# Reset learner\n",
    "custom_learner.reset()\n",
    "\n",
    "# Run non-adaptive session\n",
    "print(\"Running non-adaptive quiz...\")\n",
    "non_adaptive_result = simulator.run_quiz_session(\n",
    "    custom_learner,\n",
    "    num_questions=15,\n",
    "    quiz_type='non-adaptive',\n",
    "    tracker=tracker,\n",
    "    session_id='custom_non_adaptive'\n",
    ")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n=== RESULTS ===\")\n",
    "print(f\"\\nAdaptive:\")\n",
    "print(f\"  Accuracy: {adaptive_result['final_accuracy']:.2%}\")\n",
    "print(f\"  Mastery: {adaptive_result['mastery_index']:.3f}\")\n",
    "print(f\"  Difficulty Progression: {' → '.join(adaptive_result['difficulty_progression'][:8])}...\")\n",
    "\n",
    "print(f\"\\nNon-Adaptive:\")\n",
    "print(f\"  Accuracy: {non_adaptive_result['final_accuracy']:.2%}\")\n",
    "print(f\"  Mastery: {non_adaptive_result['mastery_index']:.3f}\")\n",
    "print(f\"  Difficulty: {non_adaptive_result['difficulty_progression'][0]} (constant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344e16a",
   "metadata": {},
   "source": [
    "## 10. Experiment with Different Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different adaptive thresholds\n",
    "thresholds = [\n",
    "    (0.70, 0.40),  # More lenient\n",
    "    (0.80, 0.50),  # Default\n",
    "    (0.90, 0.60),  # More strict\n",
    "]\n",
    "\n",
    "results_by_threshold = []\n",
    "\n",
    "for high, low in thresholds:\n",
    "    engine = AdaptiveEngine(\n",
    "        accuracy_threshold_high=high,\n",
    "        accuracy_threshold_low=low\n",
    "    )\n",
    "    \n",
    "    # Simulate performance\n",
    "    performance = [\n",
    "        {'correct': np.random.random() > 0.3, 'time': 25, 'expected_time': 30, 'difficulty': 'medium'}\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "    \n",
    "    # Track difficulty changes\n",
    "    current_diff = 'medium'\n",
    "    difficulties = [current_diff]\n",
    "    \n",
    "    for i in range(5):\n",
    "        current_diff = engine.get_next_difficulty(performance[:i+1], current_diff)\n",
    "        difficulties.append(current_diff)\n",
    "    \n",
    "    results_by_threshold.append({\n",
    "        'thresholds': f\"{high}/{low}\",\n",
    "        'progression': difficulties\n",
    "    })\n",
    "\n",
    "print(\"Difficulty Progression by Threshold:\")\n",
    "for result in results_by_threshold:\n",
    "    print(f\"\\n{result['thresholds']}: {' → '.join(result['progression'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118d5d7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates:\n",
    "- How to use LAQS components programmatically\n",
    "- Create custom learner profiles\n",
    "- Run experiments with different configurations\n",
    "- Analyze and visualize results\n",
    "- Compare adaptive vs non-adaptive approaches\n",
    "\n",
    "For a full simulation with comprehensive results, run:\n",
    "```bash\n",
    "python main.py --full\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
